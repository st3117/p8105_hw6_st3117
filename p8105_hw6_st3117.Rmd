---
title: "p8105_hw6_st3117"
author: "Sha Tao"
date: "November 15, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(rvest)
library(httr)
library(modelr)

theme_set(theme_bw())
set.seed(1)

```

## Problem 1_1 Load 'homicides' Data from Github

```{r p1_1}

p2_dataset = 
  RCurl::getURL("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv") %>%
    read_csv()

```

## Problem 1_2 Clean Data

```{r p1_2, warning = FALSE}

homicides =
  p2_dataset %>% 
  mutate(victim_race = fct_relevel(ifelse(victim_race == "White", "white", "non-white"), "white"),
         victim_age = ifelse(victim_age == "Unknown", NA, as.integer(victim_age)),
         victim_sex = as.factor(victim_sex),
         city_state = paste(paste0(city, ","), state),
         resolved = as.numeric(disposition == "Closed by arrest")) %>% 
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")) %>% 
  select(uid, victim_race, victim_age, victim_sex, city_state, resolved)

```

## Problem 1_3 Compute Odds Ratio and 95% CI for solving homicides in Baltimore

```{r p1_3}

# filter out Baltimore and do logistic regression
baltimore_logistic = 
  homicides %>%
  filter(city_state == "Baltimore, MD") %>% 
  glm(resolved ~ victim_age + victim_sex + victim_race, data = ., family = binomial()) 

# save baltimore_logistic as an object
# I got a comment for homework 5 that I did't need to actually save it as an R object, thus I'll leave it as a comment
# save(baltimore_logistic, file = "Baltimore_logistic.RData")

# compute and tidy the logistic result
baltimore_logistic %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate),
         conf.low = exp(estimate - 1.96 * std.error),
         conf.high = exp(estimate + 1.96 * std.error)) %>% 
  filter(term == "victim_racenon-white") %>% 
  select(beta = estimate, p.value, OR, conf.low, conf.high) %>% 
  knitr::kable(digit = 3)

```

## Problem 1_4 Compute Odds Ratio and 95% CI for solving homicides in All Cities

```{r p1_4}

# create function for all city logistic regression
city_logistic = function(x){
  
    homicides %>% 
    filter(city_state == x) %>% 
    glm(resolved ~ victim_age + victim_sex + victim_race, data = ., family = binomial())  %>% 
    broom::tidy() %>% 
    mutate(OR = exp(estimate),
           conf.low = exp(estimate - 1.96 * std.error),
           conf.high = exp(estimate + 1.96 * std.error)) %>% 
    filter(term == "victim_racenon-white") %>% 
    select(beta = estimate, p.value, OR, conf.low, conf.high)

}

# compute all cities' logistic regression
city_result = 
  tibble(city_state = unique(homicides$city_state)) %>% 
  mutate(map(.x = unique(homicides$city_state), ~city_logistic(.x))) %>% 
  unnest

# check some of the all cities' logistic regression result
city_result %>% 
  head() %>% 
  knitr::kable(digit = 3)

```

## Problem 1_5 Show Estimated Odds Ratio for Solving Homicides Comparing Non-white Victims to White Victims

```{r p1_5, out.width = "100%"}

city_result %>% 
  ggplot(aes(x = reorder(city_state, OR), y = OR)) +
    geom_point() +
    geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.3, size = 7)) +
    labs(
      x = "City",
      y = "Odds Ratio",
      title = "Estimated Odds Ratio for Solving Homicides Comparing Non-white to White"
    )

```

Among all cities, Tampa, FL had the highest odds ratio for solving homicides comparing non-white victims to white victims, while Boston, MA had the lowest odds ratio.\
Durham, NC had the widest 95% CI.

## Problem 2_1 Load 'Children Birth Weight' Data

```{r p2_1, message = FALSE}

children = 
  read_csv(file = "./data/birthweight.csv")

skimr::skim(children)

```

There was no missing data in the dataset. The distribution of the variables looks reasonable. Variables pnumlbw (previous number of low birth weight babies) and pnumsga (number of prior small for gestational age babies) are all 0 is probably due to the fact the there's few live births prior to this pregency in this dataset.

## Problem 2_2 Propose a regression model for birthweight

```{r p2_2, results = "hide"}

# write out the full and null model for the stepwise selection
null_model = lm(bwt ~ 1, data = children)
full_model = lm(bwt ~ ., data = children)

stepwise = step(null_model, scope = list(upper = full_model), data = children, direction = "both")

```

```{r p2_2_results}

stepwise %>% 
  broom::tidy() %>% 
  knitr::kable(digit = 3)

proposed_model1 = lm(bwt ~ bhead + blength + babysex + delwt + mrace + gaweeks, data = children)
proposed_model2 = mgcv::gam(bwt ~ s(bhead) + s(blength) + babysex + s(delwt) + mrace +s(gaweeks), data = children)

```

Step-wise selection is a data-driven model-building process, by performing it, we got 13 significant variables.
By simplily searching the association between birth weight and the 13 variables online, I narrowed it down to 6 variables: baby’s head circumference at birth, baby’s length at birth, baby’s sex, mother’s weight at delivery, mother’s race, and gestational age in weeks.

At first, I just propsed the linear model "proposed_model1", however, since the residual vs. fit below showed some violation of our linear assumption, I decided to include a non-linear "proposed_model2" with the same 6 variables.

## Problem 2_3 Residual vs. Fit Plots

```{r p2_3}

# proposed linear model
resid_fit1 = 
  children %>% 
  add_predictions(model = proposed_model1) %>% 
  add_residuals(model = proposed_model1)

ggplot(resid_fit1, aes(x = pred, y = resid)) +
    geom_point() +
    labs(
      x = "Prediction",
      y = "Residual",
      title = "Residual vs. Fit For Proposed Model 1"
    )

# proposed non-linear model
resid_fit2 = 
  children %>% 
  add_predictions(model = proposed_model2) %>% 
  add_residuals(model = proposed_model2)

ggplot(resid_fit2, aes(x = pred, y = resid)) +
    geom_point() +
    labs(
      x = "Prediction",
      y = "Residual",
      title = "Residual vs. Fit For Proposed Model 2"
    )

```

Three characteristics of a well-behaved residual vs. fits plot is

* The residuals "bounce randomly" around the 0 line.
* The residuals roughly form a "horizontal band" around the 0 line.
* No one residual "stands out" from the basic random pattern of residuals.

The residual vs. fits plot for proposed model 1 has some has some obvious outliers when the predicted birth weight less than 2000 grams. Thus, I proposed a non-linear proposed model 2 in the previous sections, and the plot has fairly random scatter pattern around the 0 line.

## Problem 2_4 Training / Testing Split 

```{r p2_4}

cv_children =
  crossv_mc(children, 100) %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble))

```

## Problem 2_5 List Columns of the Models and Their RMSE

```{r p2_5}

cv_children_test = 
  cv_children %>% 
  mutate(adjust_mod_1 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         adjust_mod_2 = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x)),
         proposed_mod1 = map(train, ~lm(bwt ~ bhead + blength + babysex + delwt + mrace + gaweeks, data = .x)),
         proposed_mod2 = map(train, ~mgcv::gam(bwt ~ s(bhead) + s(blength) + babysex +s(delwt) + mrace + 
                                                 s(gaweeks), data = .x))) %>% 
  mutate(rmse_adj_1 = map2_dbl(adjust_mod_1, test, ~rmse(model = .x, data = .y)),
         rmse_adj_2 = map2_dbl(adjust_mod_2, test, ~rmse(model = .x, data = .y)),
         rmse_propsed1 = map2_dbl(proposed_mod1, test, ~rmse(model = .x, data = .y)),
         rmse_propsed2 = map2_dbl(proposed_mod2, test, ~rmse(model = .x, data = .y)))

```

## Problem 2_6 Models' Comparison Using Violin Plot

```{r p2_6}

cv_children_test %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()

```

Based on the violin plot, our proposed model 2 has the smallest RMSE, thus I'll choose this model among the 4 models.
